{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some imports that might be useful...\n",
    "# Put them at the top so they're easy to find\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0,'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an addition  to the notes on function fitting, we consider the ideas of function fitting and model calibration in a bit more detail.\n",
    "\n",
    "We start with the example of a 'phenology' model as in the previous exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://www2.geog.ucl.ac.uk/~plewis/geogg124/_images/zhang1.png)\n",
    "\n",
    "We consider a signal (e.g. NDVI) observational data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    which tile and year?\n",
    "'''\n",
    "targetYear = 2005\n",
    "targetTile = 'h17v03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 'GDAL_DATA' not in os.environ:\n",
    "    os.environ[\"GDAL_DATA\"] = '/opt/anaconda/share/gdal'\n",
    "from get_lai import *\n",
    "\n",
    "'''\n",
    "   set up which files we want\n",
    "'''\n",
    "\n",
    "allLai = np.array(open('data/robot.txt').readlines())\n",
    "# pull date and tile info\n",
    "files = np.array([i.split('/')[-1].strip() for i in allLai])\n",
    "\n",
    "info = np.array([i.split('.')[1:3] for i in files])\n",
    "tiles = info[:,1]\n",
    "years = np.array([i[1:5] for i in info[:,0]]).astype(int)\n",
    "# filter to get just what we want\n",
    "filelist = files[((years == targetYear) & (tiles == targetTile))]\n",
    "\n",
    "'''\n",
    "    load (or not) the LAI dataset\n",
    "'''\n",
    "\n",
    "try:\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "except:\n",
    "    lai = read_lai(filelist,country='IRELAND')\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "\n",
    "'''\n",
    "    fix for what we believe are unrealistic low\n",
    "    uncertainties\n",
    "'''\n",
    "thresh = 0.25\n",
    "sd[sd<thresh] = thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and consider a logistic function that we would like to use as a model of LAI development (phenology).\n",
    "\n",
    "A (double) logistic function is:\n",
    "\n",
    "$$\n",
    " \\hat{y} = p_0 - p_1 \\left( \\frac{1}{1 + e^{p_2 (t - p_3)}} + \\frac{1}{1 + e^{p_4 (t - p_5)}} -1\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dbl_logistic_model ( p, t ):\n",
    "        \"\"\"\n",
    "            A double logistic model, as in Sobrino and Juliean, \n",
    "            or Zhang et al\n",
    "            \n",
    "            Parameters:, and rough guess\n",
    "              \n",
    "            p[0]=  ymean - 1.151*ysd  # minimum  (1.151 is 75% CI)\n",
    "            p[1] = 2*1.151*ysd        # range\n",
    "            p[2] = 0.19               # related to up slope\n",
    "            p[3] = 120                # midpoint of up slope\n",
    "            p[4] = 0.13               # related to down slope\n",
    "            p[5] = 220                # midpoint of down slope\n",
    "        \"\"\"\n",
    "        return p[0] - p[1]* ( 1./(1+np.exp(p[2]*(t-p[3]))) + \\\n",
    "                              1./(1+np.exp(-p[4]*(t-p[5])))  - 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test pixel\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "'''\n",
    "    set up x and y data for a pixel\n",
    "'''\n",
    "\n",
    "y = data[:,r,c]\n",
    "mask = ~y.mask\n",
    "y = np.array(y[mask])\n",
    "x = (np.arange(46)*8+1.)[mask]\n",
    "unc = np.array(sd[:,r,c][mask])\n",
    "x_full = np.arange(1,366) \n",
    "\n",
    "# some stats on y for rough guesses\n",
    "ysd = np.std(y)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "# some rough guesses at the parameters\n",
    "p = np.array([ymean - 1.151*ysd,2*1.151*ysd,0.1,120,0.1,240])\n",
    "\n",
    "'''\n",
    "\n",
    "Run the logistic model with these parameters\n",
    "\n",
    "'''\n",
    "y_hat = dbl_logistic_model(p,x_full)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_full,y_hat)\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the residuals. We weight the residuals by the uncertainty, to normalise and express them in 'standard deviation' units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mismatch_function(p, x, y, unc):\n",
    "    '''\n",
    "    \n",
    "    return the residuals \n",
    "    weighted by uncertainty\n",
    "    \n",
    "    '''\n",
    "    y_hat = dbl_logistic_model(p, x)\n",
    "    diff = (y_hat - y)/unc\n",
    "    return diff\n",
    "\n",
    "Z = mismatch_function(p,x,y,unc)\n",
    "\n",
    "plt.plot([1,365.],[0,0.],'k-')\n",
    "plt.xlim(0,365)\n",
    "plt.plot(x,Z,'*')\n",
    "\n",
    "print 'mean Z^2 =',(Z**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and implementing a cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, consider the observations (i.e. LAI) as Gaussian distributions $y_{obs}(x)$ ~ $N(\\bar{y}(x),C_{obs}(x))$ where $,C_{obs}(x)$ is the variance/covariance matrix (uncertainty) in the LAI observations at time $x$ and $\\bar{y}$ its mean.\n",
    "\n",
    "We assume zero covariance between the errors at each observation, so $C_{obs}(x)$ is a diagonal matrix:\n",
    "\n",
    "$\n",
    "C_{obs} = \n",
    "\\left( \\begin{array}{ccc}\n",
    "\\sigma^2_0 & 0 & ... & 0 \\\\\n",
    "0 & \\sigma^2_1 & ... & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & ... & \\sigma^2_{n-1} \\end{array} \\right) \n",
    "$\n",
    "\n",
    "over $n$ samples (in $x$) and $\\sigma^2_i$ is the variance of the $i^{th}$ sample (i.e. at $x_i$).\n",
    "\n",
    "We note that calculating the inverse of this matrix is trivial:\n",
    "\n",
    "$\n",
    "C_{obs}^{-1} = \n",
    "\\left( \\begin{array}{ccc}\n",
    "\\frac{1}{\\sigma^2_0} & 0 & ... & 0 \\\\\n",
    "0 & \\frac{1}{\\sigma^2_1} & ... & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & ... & \\frac{1}{\\sigma^2_{n-1}} \\end{array} \\right) \n",
    "$\n",
    "\n",
    "\n",
    "Suppose now that we have a *model*:\n",
    "\n",
    "$\n",
    "\\hat{y} = f(x,p)\n",
    "$\n",
    "\n",
    "where $p$ is a set of parameters of the function.\n",
    "\n",
    "We wish to obtain an *optimal* estimate of $p$, conditioned on the observations. \n",
    "\n",
    "We can do this by considering a *cost function*, $J_{obs}(y_{obs},p)$:\n",
    "\n",
    "$\n",
    "J_{obs}(y_{obs},p) = \\frac{1}{2} \\left[ y(x) - \\hat{y}(x,p) \\right]^T C_{obs}^{-1}  \\left[ y(x) - \\hat{y}(x,p) \\right]\n",
    "$\n",
    "\n",
    "where $^T$ denotes the transpose operation. \n",
    "\n",
    "We could implement a general cost function of this sort as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J_obs(p, x, y, C1,\n",
    "                      model=dbl_logistic_model):\n",
    "    '''\n",
    "    \n",
    "    return 'cost' J\n",
    "    \n",
    "    p    : set (vector) of parameters\n",
    "    x    : sample locations\n",
    "    y    : function value at sample locations\n",
    "    C1   : inverse variance /covariance matrix\n",
    "        \n",
    "    '''\n",
    "    y_hat = model(p, x)\n",
    "    \n",
    "    diff = np.matrix(y_hat - y).T\n",
    "    \n",
    "    return 0.5 * np.array(diff.T * C1 * diff).flatten()[0]\n",
    "\n",
    "\n",
    "# calculate the inverse var/covar matrix\n",
    "# NB use np matrix here\n",
    "# .I gives the inverse\n",
    "\n",
    "C1 = np.matrix(np.diag(unc**2)).I\n",
    "\n",
    "Z = J_obs(p,x,y,C1)\n",
    "\n",
    "print Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we only want to consider a diagonal uncertainty matrix here, it is simpler and faster to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J_obs_fast(p, x, y, sd,\\\n",
    "                      model=dbl_logistic_model):\n",
    "    '''\n",
    "    \n",
    "    return 'cost' J\n",
    "    \n",
    "    p    : set (vector) of parameters\n",
    "    x    : sample locations\n",
    "    y    : function value at sample locations\n",
    "    sd   : uncertainty standard deviation\n",
    "        \n",
    "    '''\n",
    "    y_hat = model(p, x)\n",
    "    \n",
    "    diff = (y_hat - y)/sd\n",
    "    \n",
    "    return 0.5 * (diff*diff).sum()\n",
    "\n",
    "\n",
    "Z = J_obs_fast(p,x,y,unc)\n",
    "\n",
    "print Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or ... we could design a function that could do either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def J_obs(p, x, y, sd,\\\n",
    "                      model=dbl_logistic_model):\n",
    "    '''\n",
    "    \n",
    "    return 'cost' J\n",
    "    \n",
    "    p    : set (vector) of parameters\n",
    "    x    : sample locations\n",
    "    y    : function value at sample locations\n",
    "    sd   : uncertainty standard deviation\n",
    "           *or* if not same shape as y, then\n",
    "           assumed to be C1 (inverse var/covar matrix)\n",
    "        \n",
    "    '''\n",
    "    y_hat = model(p, x)\n",
    "\n",
    "    if sd.shape == y.shape:\n",
    "        diff = (y_hat - y)/sd\n",
    "        J = 0.5 * (diff*diff).sum()\n",
    "    else:\n",
    "        diff = np.matrix(y_hat - y).T\n",
    "        J = 0.5 * np.array(diff.T * C1 * diff).flatten()[0]\n",
    "    \n",
    "    return J\n",
    "\n",
    "C1 = np.matrix(np.diag(unc**2)).I\n",
    "\n",
    "print J_obs(p,x,y,unc)\n",
    "print J_obs(p,x,y,C1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this example, we can change the model used for calculating $\\hat{y}$ (providing we use the same *interface*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cost using logistic model\n",
    "print J_obs(p,x,y,unc,model=dbl_logistic_model)\n",
    "\n",
    "# define another model\n",
    "\n",
    "def sincos(p,x):\n",
    "    '''\n",
    "    \n",
    "    y_hat = p[0] + p[1] sin(t) + p[2] cos(t)\n",
    "   \n",
    "    '''\n",
    "    # put x in range 0 to 2 pi and call it t\n",
    "    t = np.pi * 2 * x/365.25\n",
    "    return p[0]*np.sin(t) + p[1] * np.cos(t)\n",
    "\n",
    "# calculate the cost for this\n",
    "p2 = [0.1,0.2,0.2]\n",
    "print J_obs(p2,x,y,unc,model=sincos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 2 models we have chosen here have rather different meanings for the parameters. Indeed, the logistic takes 6 parameters (in `p`) but the `sincos` function, only 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for optimal estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider how Z varies as we vary `p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vary the first parameter\n",
    "\n",
    "pvals = np.mgrid[0.0:1.01:0.01]\n",
    "mse   = np.zeros_like(pvals)\n",
    "p_ = p.copy()\n",
    "print p\n",
    "\n",
    "for i,p0 in enumerate(pvals):\n",
    "    p_[0] = p0\n",
    "    Z = J_obs(p_,x,y,unc,model=dbl_logistic_model)\n",
    "    mse[i] = (Z**2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(pvals,np.sqrt(mse))\n",
    "plt.xlabel('p0')\n",
    "plt.ylabel('rmse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider this a 'slice' through an 'error surface' or 'cost function'. \n",
    "\n",
    "We can define the 'optimal' estimate of the parameter vector `p` as the set of `p` values that gives a *minimum* in the error surface, i.e. one for which:\n",
    "\n",
    "$\n",
    "\\frac{\\partial{J}}{\\partial{p}} = 0\n",
    "$\n",
    "\n",
    "i.e. the rate of change of the error function is zero (in all dimensions).\n",
    "\n",
    "The function $J$ in this case is 6 dimensional. It is quite hard to visualise that, but we can easily examine two dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vary the first 2 parameters\n",
    "\n",
    "pvals = np.mgrid[0.0:2.01:0.01,0.0:3.51:0.01]\n",
    "mse   = np.zeros_like(pvals[0])\n",
    "p_ = p.copy()\n",
    "\n",
    "for i in xrange(pvals.shape[1]):\n",
    "    for j in xrange(pvals.shape[2]):\n",
    "        for k in xrange(pvals.shape[0]):\n",
    "            p_[k] = pvals[k,i,j]\n",
    "            Z = J_obs(p_,x,y,unc,model=dbl_logistic_model)\n",
    "            mse[i,j] = (Z**2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow ( np.sqrt(mse), interpolation='nearest', extent=(0, 3.5, 0, 2.), origin=\"lower\",\n",
    "           cmap=plt.cm.BuPu_r)\n",
    "plt.colorbar(orientation=\"horizontal\", shrink=0.7)\n",
    "cs = plt.contour( pvals[1], pvals[0], np.sqrt(mse), [50, 100, 150, 200, 400, 600])\n",
    "plt.clabel(cs, fmt = '%2.1f', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the one and two dimensional 'slices' we can quite easily spot the minimum of the error function. Remember that this is *only* for a particular set of values of the parameters that we do not vary here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look up table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to finding the global minimum would be to simply calculate $J$ at some set of points (e.g. on a six dimensional grid), and to see which point showed the minimum of $J$. We can do this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    " recall the meaning of the parameters for the model we are using\n",
    "\n",
    " p[0]=  ymean - 1.151*ysd  # minimum  (1.151 is 75% CI) of LAI\n",
    " p[1] = 2*1.151*ysd        # range of LAI\n",
    " p[2] = 0.19               # related to up slope\n",
    " p[3] = 120                # midpoint of up slope\n",
    " p[4] = 0.13               # related to down slope\n",
    " p[5] = 220                # midpoint of down slope\n",
    "\n",
    "'''\n",
    "\n",
    "# consider parameter bounds\n",
    "\n",
    "b = np.array([(0.01,10.),(0.01,10.),(0.01,1.),\\\n",
    "                  (50.,300.),(0.01,1.),(50.,300.)])\n",
    "\n",
    "# number of samples per dimension (+1)\n",
    "N = 5\n",
    "\n",
    "# work out what steps to use over the grid\n",
    "s = [(bound[1]-bound[0])/float(N) for bound in b]\n",
    "# form a grid of samples over parameter space\n",
    "\n",
    "pvals = np.mgrid[b[0,0]:b[0,1]+s[0]:s[0],\\\n",
    "                 b[1,0]:b[1,1]+s[1]:s[1],\\\n",
    "                 b[2,0]:b[2,1]+s[2]:s[2],\\\n",
    "                 b[3,0]:b[3,1]+s[3]:s[3],\\\n",
    "                 b[4,0]:b[4,1]+s[4]:s[4],\\\n",
    "                 b[5,0]:b[5,1]+s[5]:s[5]]\n",
    "\n",
    "# set sse default to high\n",
    "sse   = np.zeros_like(pvals[0]) + 1e20\n",
    "p_ = p.copy()\n",
    "\n",
    "print 'the grid shape is',pvals.shape\n",
    "print 'the LUT grid has',pvals.size,'entries'\n",
    "\n",
    "for i0 in xrange(pvals.shape[1]):\n",
    "  for i1 in xrange(pvals.shape[2]):\n",
    "    for i2 in xrange(pvals.shape[3]):\n",
    "      for i3 in xrange(pvals.shape[4]):\n",
    "        for i4 in xrange(pvals.shape[5]):\n",
    "          for i5 in xrange(pvals.shape[6]):\n",
    "\n",
    "            for k in xrange(pvals.shape[0]):\n",
    "              p_[k] = pvals[k,i0,i1,i2,i3,i4,i5]\n",
    "              \n",
    "              try:\n",
    "                Z = J_obs(p_,x,y,unc,model=dbl_logistic_model)\n",
    "                sse[i0,i1,i2,i3,i4,i5] = (Z**2).sum()\n",
    "              except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but this takes a relatively long time, *even* if we use a really coarse grid as in this example (with a few hundred thousand samples).\n",
    "\n",
    "That said, LUT inversion is a pragmatic response to trying to find an optimal solution for the parameters when fitting a model. It is viable in low dimensional space, but becomes problematic as the number of dimensions increases. \n",
    "\n",
    "Note also that, as it stands, it can only provide a rough estimate of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Seek the minimum SSE\n",
    "\n",
    "'''\n",
    "# find the minimum value over the grid\n",
    "minSSE = np.min(sse)\n",
    "print 'the minimum value is',minSSE\n",
    "\n",
    "# find where that occurs\n",
    "w = np.where(sse == minSSE )\n",
    "print '\\nthe min is at sample',w\n",
    "\n",
    "# a bit awkward to pull this out but it is possible\n",
    "result = np.array(pvals[:,w[0],w[1],w[2],w[3],w[4],w[5]]).T[0]\n",
    "\n",
    "print '\\nthe result is',result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actually, we would typically use np.argmin\n",
    "# for this\n",
    "\n",
    "minsamp = np.argmin(np.array(sse))\n",
    "\n",
    "# now extract that for each parameter\n",
    "result = np.array([pvals[i].flatten()[minsamp] \\\n",
    "                   for i in xrange(pvals.shape[0])])\n",
    "\n",
    "print '\\nthe result is',result \n",
    "\n",
    "# so we see we get the same result, whichever approach we take"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now plot the data and model with these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = dbl_logistic_model(result,x_full)\n",
    "\n",
    "plt.plot(x_full,y_hat)\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that it is not a bad effort (in that it broadly describes the data), but we do not know whether a 'better' (lower J) solution could have been obtained in between the grid points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A faster LUT\n",
    "\n",
    "In the code snippet above, when creating the LUT, we can see that we have many nested loops. Generally speaking, this is a bad idea. We have also seen that the code is very slow. We can try to do the same here using some tricks to speed things up. In Python (as well as R, and to some extent, matlab and others), speed is usually hampered by having lots of ntested loops. We can usually do away with them. \n",
    "\n",
    "One way of creating a LUT efficiently is to note that if we have the same sample points along each dimension, the LUT is just the *cartesian product* of the (in this case 6) different dimensions. Here's the cartesian product for two dimensions:\n",
    "<p><a href=\"https://commons.wikimedia.org/wiki/File:Cartesian_Product_qtl1.svg#/media/File:Cartesian_Product_qtl1.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4e/Cartesian_Product_qtl1.svg\" alt=\"Cartesian Product qtl1.svg\" height=\"145\" width=\"191\"></a><br>By <a href=\"//commons.wikimedia.org/wiki/User:Quartl\" title=\"User:Quartl\">Quartl</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, <a href=\"http://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=22436861\">Link</a></p>\n",
    "\n",
    "The Python [itertools](http://jmduke.com/posts/a-gentle-introduction-to-itertools/) module has this functionality implemented in the [`product`](https://docs.python.org/2/library/itertools.html#itertools.product) method. So we just need to create one list or array per dimension with the sampling points, and then use `itertools.product` to create our LUT.\n",
    "\n",
    "We can then use the python `map` method, which applies each element in an iterator (e.g. each row of the LUT) to a function. The function can only take  one parameter, so we have to *alias* `J_obs` using a `lambda` function that fixes the additional parameters in the function call. The result of this is an array with the cost associated with each parameter set in the LUT.\n",
    "\n",
    "We can use the `argmin` method to find the location of the minimum, and then use that position in the LUT array to get the parameters that get the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# The parameter boundaries\n",
    "b = np.array([(0.01,10.),(0.01,10.),(0.01,1.),\n",
    "                  (50.,300.),(0.01,1.),(50.,300.)])\n",
    "print b.shape\n",
    "import itertools\n",
    "\n",
    "n_samples =10 # 5 samples by dimension\n",
    "pre_lut = np.zeros((6,n_samples)) # For each dimension, we store one vector with n_samples\n",
    "for i in xrange(6):\n",
    "    pre_lut[i,:] = np.linspace ( b[i,0], b[i,1], n_samples)\n",
    "    \n",
    "# Generates the LUT by doing a cartesian product of the 6 dimensions\n",
    "LUT = [xx for xx in itertools.product ( pre_lut[0], pre_lut[1], \n",
    "                                       pre_lut[2], pre_lut[3], pre_lut[4], pre_lut[5])]\n",
    "LUT = np.array(LUT) # Convert to array\n",
    "print LUT.shape\n",
    "\n",
    "# Calculate the cost function\n",
    "\n",
    "cost_f = lambda pp: J_obs(pp,x,y,unc,model=dbl_logistic_model)**2 \n",
    "cost = map (cost_f, LUT)\n",
    "cost = np.array (cost)\n",
    "iloc = cost.argmin()\n",
    "print cost[iloc]\n",
    "print LUT[iloc, :]\n",
    "y_hat = dbl_logistic_model(LUT[iloc,:],x_full)\n",
    "\n",
    "plt.plot(x_full,y_hat)\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting observation is that most of the time is spend evaluating the cost function (the creating of the LUT is quite fast), and that most modern computers have several CPUs. The calculation of the cost function for each element of the LUT is completely independent (the calculation of element 1 and element 100 is completely independent). We could evaluate these functions *in parallel*. To do that, we can use the [`multiprocessing`](https://pymotw.com/2/multiprocessing/basics.html) module. Once we import the `Pool` and define it (it's basically a pool of \"*workers*\" with the size of cores in your computer), then a map method will spread the calculations over the workers. As before, we write a wrapper function to ignore the extra nuisance parameters in `J_obs`, but in this case using a real function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def cost_f2 (pp):\n",
    "    return J_obs(pp,x,y,unc,model=dbl_logistic_model)**2 \n",
    "\n",
    "\n",
    "pool = Pool()\n",
    "print \"Multicore:\"\n",
    "%time costf = np.array(pool.map(cost_f2, LUT))\n",
    "\n",
    "print \"Singlecore\"\n",
    "%time cost = np.array(map (cost_f, LUT))\n",
    "\n",
    "print np.allclose (cost, costf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, this provides a speedup of around 3x.\n",
    "\n",
    "<div style=\"padding: 20px;\n",
    "    background-color: #f44336;\n",
    "    color: white;\n",
    "    font-size: 130%\">\n",
    "Warning! If you use multiprocessing, you will use all of the computer resources. Be gentle with other users, and use <tt>nice -19 jupyter notebook</tt> when launching the notebook (see <a href=\"https://www.cyberciti.biz/faq/change-the-nice-value-of-a-process/\">here</a> for more information).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have not used the fact in the example above, the real benefit of a LUT is that you can *pre-calculate* the results at each of the grid points. This means that if you had a computationally expensive model, you could calculate $\\hat{y}(x)$ for all grid points (note that this would be quite a large dataset). \n",
    "\n",
    "Implement a LUT optimisation approach (with a smaller grid than that used above). To do this:\n",
    "\n",
    "1. calculate the grid points, similarly to above (but perhaps a bit smaller)\n",
    "2. calculate and store $\\hat{y}(x)$ for each grid point.\n",
    "3. Calculate the cost $J$ between a particular observation (i.e. the values of LAI for a given pixel) and the LUT values (normalise dby uncertainty)\n",
    "4. Find the minimum over the LUT and the associated parameter values\n",
    "\n",
    "You should then plot your results as above. \n",
    "\n",
    "Since we have to calculate $\\hat{y}(x)$ at all grid points, generating the LUT might take a few minutes. \n",
    "\n",
    "What then is the advantage of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a minimisation routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are sometimes great practical advantages to using a LUT, there can be distinct downsides too (e.g. quantisation).\n",
    "\n",
    "Another approach is that we use some 'algorithm' that seeks the N-dimensional minimum by 'moving' around the parameter space.\n",
    "\n",
    "One such approach, and generally a useful algorithm is [L-BFGS-B](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "p = result\n",
    "\n",
    "# initial estimate is in p\n",
    "print 'initial parameters:',p[0],p[1],p[2],p[3],p[4],p[5]\n",
    "\n",
    "# set some bounds for the parameters\n",
    "bound = np.array([(0.01,10.),(0.01,10.),(0.01,1.),\\\n",
    "                  (50.,300.),(0.01,1.),(50.,300.)])\n",
    "\n",
    "\n",
    "\n",
    "def sse(p,x,y,unc):\n",
    "    '''Sum of squared error'''\n",
    "    # penalise p[3] > p[5]\n",
    "    err = np.max([0.,(p[3] - p[5])])*1e4\n",
    "    return J_obs(p,x,y,unc,model=dbl_logistic_model)+err\n",
    "\n",
    "# we pass the function:\n",
    "#\n",
    "# sse               : the name of the function we wrote to give \n",
    "#                     sum of squares of Z_i\n",
    "# p                 : an initial estimate of the parameters\n",
    "# args=(x,y,unc)    : the other information (other than p) that\n",
    "#                     mismatch_function needs\n",
    "# approx_grad       : if we dont have a function for the gradien\n",
    "#                     we have to get the solver to approximate it\n",
    "#                     which takes time ... see if you can work out\n",
    "#                     d_sse / dp and use that to speed this up!\n",
    "\n",
    "psolve = optimize.fmin_l_bfgs_b(sse,p,approx_grad=True,iprint=1,\\\n",
    "                                args=(x,y,unc),bounds=bound)\n",
    "\n",
    "print psolve[1]\n",
    "pp = psolve[0]\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)\n",
    "y_hat = dbl_logistic_model(pp,x_full)\n",
    "plt.plot(x_full,y_hat)\n",
    "\n",
    "print 'solved parameters: ',pp[0],pp[1],pp[2],pp[3],pp[4],pp[5]\n",
    "\n",
    "print '\\nJ(min) = ',J_obs(p,x,y,unc,model=dbl_logistic_model)\n",
    "\n",
    "# if we define the phenology as the parameter p[3]\n",
    "# and the 'length' of the growing season:\n",
    "print '\\nphenology',pp[3],pp[5]-pp[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives a much more plausible result and a *much* lower SSE and took around 100 iterations (so a few hundred calculations of the cost function). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what the advantages and disadvantages of this sort of approach might be (e.g. compared to the LUT) (e.g. depending on whther there are *many* models to fit or only a few).\n",
    "\n",
    "The uncertainty in the parameters can be obtained from the Hessian of the cost function. See if you can work out how that might be calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
