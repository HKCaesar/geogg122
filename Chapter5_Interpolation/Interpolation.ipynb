{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Function fitting and Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's session, we will be using some of the LAI datasets we examined last week (masked by national boundaries) and doing some analysis on them.\n",
    "\n",
    "- [5.1 Making 3D datasets and Movies](#5.1-Making-3D-datasets-and-Movies)\n",
    "    First, we will examine how to improve our data reading function by extracting only the area we are interested in. This involves querying the 'country' mask to find its limits and passing this information through to the reader.\n",
    "\n",
    "- [5.2 Interpolation](#5.2-Interpolation)\n",
    "    Then we will look at methods to interpolate and smooth over gaps in datasets using various methods.\n",
    "\n",
    "- [5.3 Function Fitting](#5.3-Function-fitting)\n",
    "    Finally, we will look at fitting models to datasets, in this case a model describing LAI phenology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Making 3D datasets and Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First though, we will briefly go over once more the work we did on downloading data (ussssing `wget`), generating 3D masked datasets, and making movies.\n",
    "\n",
    "This time, we will concentrate more on generating functions that we can re-use for other purposes.\n",
    "\n",
    "### 5.1.1 Downloading data\n",
    "\n",
    "We start by writing some code [getModis.py](python/getModis.py) to pull a particular MODIS LAI file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006//2013.02.18/MCD15A2H.A2013049.h31v11.006.2015256194138.hdf\n",
      "MCD15A2H.A2013049.h31v11.006.2015256194138.hdf\n"
     ]
    }
   ],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import urllib2\n",
    "import numpy as np\n",
    "import wget\n",
    "\n",
    "'''\n",
    "Bare bones of a script to get MODIS dataset\n",
    "'''\n",
    "\n",
    "# url found from reverb ordering\n",
    "url = 'http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/'\n",
    "tile = 'h31v11'\n",
    "dates = '2013.02.18'\n",
    "\n",
    "# read htl of base url\n",
    "response = urllib2.urlopen(url + dates)\n",
    "html = response.read()\n",
    "# see https://docs.python.org/2/library/htmlparser.html#module-HTMLParser\n",
    "# parse the html to get data\n",
    "# check data characteristics are ok\n",
    "# and form geturl to download with wget\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_data(self, data):\n",
    "      if len(data):\n",
    "        sdata = data.split()\n",
    "        if len(sdata) == 1:\n",
    "          fdata = sdata[0].split('.')\n",
    "          if (fdata[-1] == 'hdf') and (fdata[2] == tile):\n",
    "            geturl = url + '/'  + dates + '/' + data\n",
    "            # pull dataset\n",
    "            print geturl\n",
    "            filename = wget.download(geturl)\n",
    "            print filename\n",
    "parser = MyHTMLParser()\n",
    "parser.feed(html)\n",
    "\n",
    "# ProfLewis\n",
    "# GeogG1222016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have access to `wget`, you will need to install it.\n",
    "\n",
    "Using anaconda python, [we can type](http://conda.pydata.org/docs/using/envs.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/plewis/anaconda/bin/anaconda\n",
      "# conda environments:\n",
      "#\n",
      "my_root                  /Users/plewis/anaconda/envs/my_root\n",
      "root                  *  /Users/plewis/anaconda\n",
      "\n",
      "Fetching package metadata .......\n",
      "Solving package specifications: ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CondaValueError: Value error: prefix already exists: /Users/plewis/anaconda/envs/my_root\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "InstallError: Install error: Error: one or more of the packages already installed depend on 'conda'\n",
      "and should only be installed in the root environment: conda-env\n",
      "These packages need to be removed before conda can proceed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# which version of anaconda do you pick up?\n",
    "which anaconda\n",
    "\n",
    "# which environments are set?\n",
    "conda info --envs\n",
    "\n",
    "# create a user environment called my_root\n",
    "# unless you have already done this\n",
    "\n",
    "conda create --name=my_root --clone=/opt/anaconda\n",
    "\n",
    "# you may get \n",
    "# Error: prefix already exists: /home/samiam/.conda/envs/my_root\n",
    "# if this exists\n",
    "# Also, if your anaconda distribution is elsewhere eg /Users/plewis/anaconda\n",
    "# then you may need\n",
    "# conda create --name=my_root --clone=/Users/plewis/anaconda\n",
    "\n",
    "source activate my_root\n",
    "\n",
    "# try an update conda?\n",
    "# conda update --all\n",
    "# and perhaps\n",
    "# conda clean --all\n",
    "\n",
    "# get the wget package\n",
    "conda install wget\n",
    "\n",
    "# you should now restart the kernel to make sure this is active in this\n",
    "# notebook, and re-run the first cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 read multiple dates\n",
    "\n",
    "we can wrap a method around the code above to get data for a singgle date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006//2013.02.18/MCD15A2H.A2013049.h31v11.006.2015256194138.hdf\n"
     ]
    }
   ],
   "source": [
    "def getMODIS(url = 'http://e4ftl01.cr.usgs.gov//MODV6_Cmp_A/MOTA/MCD15A2H.006/',\\\n",
    "            tile = 'h31v11',date = '2013.02.18'):\n",
    "\n",
    "    # read htl of base url\n",
    "    response = urllib2.urlopen(url + date)\n",
    "    html = response.read()\n",
    "    # see https://docs.python.org/2/library/htmlparser.html#module-HTMLParser\n",
    "    # parse the html to get data\n",
    "    # check data characteristics are ok\n",
    "    # and form geturl to download with wget\n",
    "    \n",
    "    # should check to see if file already exists\n",
    "\n",
    "    class MyHTMLParser(HTMLParser):\n",
    "        def handle_data(self, data):\n",
    "          if len(data):\n",
    "            sdata = data.split()\n",
    "            if len(sdata) == 1:\n",
    "              fdata = sdata[0].split('.')\n",
    "              if (fdata[-1] == 'hdf') and (fdata[2] == tile):\n",
    "                geturl = url + '/'  + dates + '/' + data\n",
    "                # pull dataset\n",
    "                print geturl\n",
    "                filename = wget.download(geturl)\n",
    "                self.filename = filename\n",
    "    parser = MyHTMLParser()\n",
    "    parser.feed(html)\n",
    "    return parser.filename\n",
    "    \n",
    "filename = getMODIS(date = '2013.02.18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Read Just The Data We Want\n",
    "\n",
    "Last time, we generated a function to read MODIS LAI data.\n",
    "\n",
    "We have now included such a function in the directory [`files/python`](files/python) called [`get_lai.py`](files/python/get_lai.py).\n",
    "\n",
    "The only added sophistication is that when we call `ReadAsArray`, we give it the starting cols, rows, and number of cols and rows to read (e.g. `xsize=600,yoff=300,xoff=300,ysize=600`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we have a list of filenames\n",
    "# load read_lai\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "\n",
    "from get_lai import get_lai\n",
    "\n",
    "help(get_lai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g. for reading a single file:\n",
    "\n",
    "lai_file0 = get_lai('data/%s'%filelist[20],ncol=600,mincol=300,minrow=400,nrow=800)\n",
    "plt.imshow(lai_file0['Lai_1km'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(lai_file0)\n",
    "print lai_file0.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary with has keys `['Lai_1km', 'LaiStdDev_1km', 'FparLai_QC']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lai_file0['Lai_1km'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these datasets is of shape `(1200, 1200)`, but we have read only 600 (columns) and 800 (rows) in this case. Note that the numpy indexing is `(rows,cols)`.\n",
    "\n",
    "We know how to create a mask from a vector dataset from thelast session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have to make sure have access to gdal data files \n",
    "import os\n",
    "if 'GDAL_DATA' not in os.environ:\n",
    "    os.environ[\"GDAL_DATA\"] = '/opt/anaconda/share/gdal'\n",
    "\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "# make a raster mask\n",
    "# from the layer IRELAND in world.shp\n",
    "filename = filelist[0]\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template%('data/%s'%filename,'Lai_1km')\n",
    "                           \n",
    "mask = raster_mask(file_spec,\\\n",
    "                   target_vector_file = \"data/world.shp\",\\\n",
    "                   attribute_filter = \"NAME = 'IRELAND'\")\n",
    "\n",
    "\n",
    "plt.imshow(mask)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the data we want is only a small section of the whole spatial dataset.\n",
    "\n",
    "It would be convenient to extract *only* the part we want.\n",
    "\n",
    "We can use `numpy.where()` to help with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The mask is False for the area we want\n",
    "rowpix,colpix = np.where(mask == False)\n",
    "\n",
    "print rowpix,colpix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rowpix` and `colpix` are lists of pixel coordinates where the condition we specified is `True` (i.e. where `mask` is `False`).\n",
    "\n",
    "If we wanted to find the bounds of this area, we simply need to know the minimum and maximum column and row in these lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mincol,maxcol = min(colpix),max(colpix)\n",
    "minrow,maxrow = min(rowpix),max(rowpix)\n",
    "\n",
    "# think about why the + 1 here!!!\n",
    "# what if maxcol and mincol were the same?\n",
    "ncol = maxcol - mincol + 1\n",
    "nrow = maxrow - minrow + 1\n",
    "\n",
    "print minrow,mincol,nrow,ncol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this information to extract *only* the area we want when we read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lai_file0 = get_lai('data/%s'%filelist[20],\\\n",
    "                    ncol=ncol,nrow=nrow,mincol=mincol,minrow=minrow)\n",
    "\n",
    "plt.imshow(lai_file0['Lai_1km'],interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets extract this portion of the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "plt.imshow(small_mask,interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And combine the country mask with the small dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a recap, we can use the function `raster_mask` that we gave you last time to develop a raster mask (!) from an ESRI shapefile (`data/world.shp` here).\n",
    "\n",
    "We can then combine this mask with the QC-derived mask in the LAI dataset.\n",
    "\n",
    "The LAI mask (that will be `lai.mask` in the code below) is `False` for good data, as is the coutry mask.\n",
    "\n",
    "To combine them, we want some operator `X` for which:\n",
    "\n",
    "`True  X True  == True`  \n",
    "`True  X False == True`  \n",
    "`False X True  == True`  \n",
    "`False X False == False`  \n",
    "\n",
    "The operator to use then is an *or*, here, a bitwise or, `|`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lai_file0 = get_lai('data/%s'%filelist[20],\\\n",
    "                    ncol=ncol,nrow=nrow,mincol=mincol,minrow=minrow)\n",
    "\n",
    "layer = 'Lai_1km'\n",
    "lai = lai_file0[layer]\n",
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "# combined mask\n",
    "new_mask = small_mask | lai.mask\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(new_mask,interpolation='none')\n",
    "\n",
    "lai = ma.array(lai,mask=new_mask)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(lai,interpolation='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be used to writing loops around such functions.\n",
    "\n",
    "In this case, we read *all* of the files in `filelist` and put the data into the dictionary called `lai` here.\n",
    "\n",
    "Because there are multiple layers in the datasets, we loop over layer and append to each list indiviually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load 'em all ...\n",
    "\n",
    "# for United Kingdom here\n",
    "\n",
    "import numpy.ma as ma\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "country = 'UNITED KINGDOM'\n",
    "\n",
    "# make a raster mask\n",
    "# from the layer UNITED KINGDOM in world.shp\n",
    "filename = filelist[0]\n",
    "file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "file_spec = file_template%('data/%s'%filename,'Lai_1km')\n",
    "                           \n",
    "mask = raster_mask(file_spec,\\\n",
    "                   target_vector_file = \"data/world.shp\",\\\n",
    "                   attribute_filter = \"NAME = '%s'\"%country)\n",
    "# extract just the area we want\n",
    "# by getting the min/max rows/cols\n",
    "# of the data mask\n",
    "# The mask is False for the area we want\n",
    "rowpix,colpix = np.where(mask == False)\n",
    "mincol,maxcol = min(colpix),max(colpix)\n",
    "minrow,maxrow = min(rowpix),max(rowpix)\n",
    "ncol = maxcol - mincol + 1\n",
    "nrow = maxrow - minrow + 1\n",
    "# and make a small mask\n",
    "small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "\n",
    "\n",
    "# data_fields with empty lists\n",
    "data_fields = {'LaiStdDev_1km':[],'Lai_1km':[]}\n",
    "\n",
    "# make a dictionary and put the filenames in it\n",
    "# along with the mask and min/max info\n",
    "lai = {'filenames':np.sort(filelist),\\\n",
    "       'minrow':minrow,'mincol':mincol,\\\n",
    "       'mask':small_mask}\n",
    "\n",
    "# combine the dictionaries\n",
    "lai.update(data_fields)\n",
    "\n",
    "# loop over each filename\n",
    "for f in np.sort(lai['filenames']):\n",
    "    this_lai = get_lai('data/%s'%f,\\\n",
    "                       mincol=mincol,ncol=ncol,\\\n",
    "                       minrow=minrow,nrow=nrow)\n",
    "    for layer in data_fields.keys():\n",
    "        # apply the mask\n",
    "        new_mask = this_lai[layer].mask | small_mask\n",
    "        this_lai[layer] = ma.array(this_lai[layer],mask=new_mask)\n",
    "        lai[layer].append(this_lai[layer])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have a look at one of these\n",
    "\n",
    "i = 20\n",
    "\n",
    "import pylab as plt\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'][i].shape\n",
    "\n",
    "root = 'images/lai_uk'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "f = lai['filenames'][i]\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "# get some info from filename\n",
    "file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "print file_id\n",
    "plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "# plot a jpg\n",
    "plt.title(file_id)\n",
    "plt.colorbar()\n",
    "plt.savefig('images/lai_uk_%s.jpg'%file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thats quite good, so put as a function:\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "from get_lai import get_lai\n",
    "from raster_mask import raster_mask\n",
    "\n",
    "\n",
    "def read_lai(filelist,datadir='data',country=None):\n",
    "    '''\n",
    "    Read MODIS LAI data from a set of files\n",
    "    in the list filelist. Data assumed to be in\n",
    "    directory datadir.\n",
    "    \n",
    "    Parameters:\n",
    "    filelist : list of LAI files\n",
    "    \n",
    "    Options:\n",
    "    datadir  : data directory\n",
    "    country  : country name (in data/world.shp)\n",
    "    \n",
    "    Returns:\n",
    "    lai dictionary\n",
    "    '''\n",
    "    if country:\n",
    "        # make a raster mask\n",
    "        # from the layer UNITED KINGDOM in world.shp\n",
    "        file_template = 'HDF4_EOS:EOS_GRID:\"%s\":MOD_Grid_MOD15A2:%s'\n",
    "        file_spec = file_template%('data/%s'%filelist[0],'Lai_1km')\n",
    "                                   \n",
    "        mask = raster_mask(file_spec,\\\n",
    "                           target_vector_file = \"data/world.shp\",\\\n",
    "                           attribute_filter = \"NAME = '%s'\"%country)\n",
    "        # extract just the area we want\n",
    "        # by getting the min/max rows/cols\n",
    "        # of the data mask\n",
    "        # The mask is False for the area we want\n",
    "        rowpix,colpix = np.where(mask == False)\n",
    "        mincol,maxcol = min(colpix),max(colpix)\n",
    "        minrow,maxrow = min(rowpix),max(rowpix)\n",
    "        ncol = maxcol - mincol + 1\n",
    "        nrow = maxrow - minrow + 1\n",
    "        # and make a small mask\n",
    "        small_mask = mask[minrow:minrow+nrow,mincol:mincol+ncol]\n",
    "    else:\n",
    "        # no country\n",
    "        mincol = 0\n",
    "        maxcol = 0\n",
    "        ncol = None\n",
    "        nrow = None\n",
    "\n",
    "    # data_fields with empty lists\n",
    "    data_fields = {'LaiStdDev_1km':[],'Lai_1km':[]}\n",
    "    \n",
    "    # make a dictionary and put the filenames in it\n",
    "    # along with the mask and min/max info\n",
    "    lai = {'filenames':np.sort(filelist),\\\n",
    "           'minrow':minrow,'mincol':mincol,\\\n",
    "           'mask':small_mask}\n",
    "    \n",
    "    # combine the dictionaries\n",
    "    lai.update(data_fields)\n",
    "    \n",
    "    # loop over each filename\n",
    "    for f in np.sort(lai['filenames']):\n",
    "        this_lai = get_lai('data/%s'%f,\\\n",
    "                           mincol=mincol,ncol=ncol,\\\n",
    "                           minrow=minrow,nrow=nrow)\n",
    "        for layer in data_fields.keys():\n",
    "            # apply the mask\n",
    "            if country:\n",
    "                new_mask = this_lai[layer].mask | small_mask\n",
    "                this_lai[layer] = ma.array(this_lai[layer],mask=new_mask)\n",
    "            lai[layer].append(this_lai[layer])   \n",
    "    for layer in data_fields.keys():\n",
    "        lai[layer] = ma.array(lai[layer])\n",
    "            \n",
    "    return lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test this ... the one in the file\n",
    "# does a cutout of the data area as well\n",
    "# which will keep the memory\n",
    "# requirements down\n",
    "from get_lai import read_lai\n",
    "\n",
    "lai = read_lai(filelist,country='IRELAND',verbose=True)\n",
    "\n",
    "# have a look at one of these\n",
    "\n",
    "i = 20\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'][i].shape\n",
    "\n",
    "root = 'images/lai_eire'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "f = lai['filenames'][i]\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "# get some info from filename\n",
    "file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "print file_id\n",
    "plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "# plot a jpg\n",
    "plt.title(file_id)\n",
    "plt.colorbar()\n",
    "plt.savefig('%s_%s.jpg'%(root,file_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a movie\n",
    "\n",
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "# just see what the shape is ...\n",
    "print lai['Lai_1km'].shape\n",
    "\n",
    "root = 'images/lai_country_eire'\n",
    "\n",
    "cmap = plt.cm.Greens\n",
    "\n",
    "for i,f in enumerate(lai['filenames']):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    # get some info from filename\n",
    "    file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "    print file_id\n",
    "    plt.imshow(lai['Lai_1km'][i],cmap=cmap,interpolation='none',vmax=4.,vmin=0.0)\n",
    "    # plot a jpg\n",
    "    plt.title(file_id)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "    plt.close(fig)\n",
    "    \n",
    "cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie.gif'.format(root)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_country_eire_movie.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The movie making works, so pack that into a function\n",
    "\n",
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "root = 'images/lai_eire'\n",
    "\n",
    "def make_movie(lai,root,layer='Lai_1km',vmax=4.,vmin=0.,do_plot=False):\n",
    "    '''\n",
    "    Make an animated gif from MODIS LAI data in\n",
    "    dictionary 'lai'.\n",
    "    \n",
    "    Parameters:\n",
    "    lai    : data dictionary\n",
    "    root   : root file /directory name of frames and movie\n",
    "    \n",
    "    layer  : data layer to plot \n",
    "    vmax   : max value for plotting\n",
    "    vmin   : min value for plotting\n",
    "    do_plot: set True if you want the individual plots\n",
    "             to display\n",
    "    \n",
    "    Returns:\n",
    "    movie name    \n",
    "    \n",
    "    '''\n",
    "    cmap = plt.cm.Greens\n",
    "    \n",
    "    for i,f in enumerate(lai['filenames']):\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        # get some info from filename\n",
    "        file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "        print file_id\n",
    "        plt.imshow(lai[layer][i],cmap=cmap,interpolation='none',\\\n",
    "                   vmax=vmax,vmin=vmin)\n",
    "        # plot a jpg\n",
    "        plt.title(file_id)\n",
    "        plt.colorbar()\n",
    "        plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "        if not do_plot:\n",
    "            plt.close(fig)\n",
    "        \n",
    "    cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie.gif'.format(root)\n",
    "    os.system(cmd)\n",
    "    return '{0}_movie.gif'.format(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it\n",
    "\n",
    "lai_uk = read_lai(filelist,country='UNITED KINGDOM')\n",
    "root = 'images/lai_UK'\n",
    "movie = make_movie(lai_uk,root)\n",
    "print movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_UK_movie.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Univariate interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can load the data we want from multiple MODIS hdf files that we have downloaded from the NASA server into a 3D masked numpy array, with a country boundary mask (projected int the raster data coordinate system) from a vector dataset.\n",
    "\n",
    "Let's start to explore the data then.\n",
    "\n",
    "You should have an array of LAI for Ireland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(lai['Lai_1km'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the LAI for some given pixels.\n",
    "\n",
    "First, we might like to identify which pixels actually have any data.\n",
    "\n",
    "A convenient function for this would be `np.where` that returns the indices of items that are `True`.\n",
    "\n",
    "Since the data mask is `False` for good data, we take the complement `~` so that good data are `True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = lai['Lai_1km']\n",
    "np.where(~data.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example good pixel this is (3,329,145). Let's look at this for all time periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = lai['Lai_1km']\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(np.arange(len(pixel))*8,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(np.arange(len(pixel))*8,pixel,'k--')\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data follow the trend of what we might expect for LAI development, but they are clearly a little noisy.\n",
    "\n",
    "We also have access to uncertainty information (standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy the data in case we change it any\n",
    "\n",
    "data = lai['Lai_1km'].copy()\n",
    "sd   = lai['LaiStdDev_1km'].copy()\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel    = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(len(pixel))*8\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(x,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(x,pixel,'k--')\n",
    "# plot error bars:\n",
    "# 1.96 because that is the 95% confidence interval\n",
    "plt.errorbar(x,pixel,yerr=pixel_sd*1.96)\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would generally expect LAI to be quite smoothly varying over time. Visualising the data with 95% confidence intervals is quite useful as we can now 'imagine' some smooth line that would generally go within these bounds.\n",
    "\n",
    "Some of the uncertainty estimates are really rather small though, which are probably not reliable.\n",
    "\n",
    "Let's inflate them:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data = lai['Lai_1km'].copy()\n",
    "sd   = lai['LaiStdDev_1km'].copy()\n",
    "\n",
    "r = 329\n",
    "c = 83\n",
    "\n",
    "pixel    = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "# threshold\n",
    "thresh = 0.25\n",
    "pixel_sd[pixel_sd<thresh] = thresh\n",
    "\n",
    "x = np.arange(len(pixel))*8\n",
    "\n",
    "# plot red stars at the data points\n",
    "plt.plot(x,pixel,'r*')\n",
    "# plot a black (k) dashed line (--)\n",
    "plt.plot(x,pixel,'k--')\n",
    "# plot error bars:\n",
    "# 1.96 because that is the 95% confidence interval\n",
    "plt.errorbar(x,pixel,yerr=pixel_sd*1.96)\n",
    "plt.xlabel('doy')\n",
    "plt.ylabel('LAI')\n",
    "plt.title('pixel %03d %03d'%(r,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps a bit more realistic ...\n",
    "\n",
    "The data now have some missing values (data gaps) and, as we have noted, are a little noisy.\n",
    "\n",
    "A Python module we can use for many scientific functions is [`scipy`](http://docs.scipy.org/doc/scipy), in particular here, the [`scipy` interpolation functions](http://docs.scipy.org/doc/scipy/reference/interpolate.html).\n",
    "\n",
    "We need to make a careful choice of the interpolation functions.\n",
    "\n",
    "We might, in many circumstances simply want something that interpolates between data points, i.e. that goes through the data points that we have.\n",
    "\n",
    "Many interpolators will not provide extrapolation, so in the example above we could not get an estimate of LAI prior to the first sample and after the last.\n",
    "\n",
    "The best way to deal with that would be to have multiple years of data.\n",
    "\n",
    "Instead here, we will repeat the dataset three times to mimic this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "\n",
    "# original x,y\n",
    "y_ = pixel\n",
    "x_ = (np.arange(len(y_))*8.+1)[~pixel.mask]\n",
    "y_ = y_[~pixel.mask]\n",
    "\n",
    "# extend: using np.tile() to repeat data\n",
    "y_extend = np.tile(y_,3)\n",
    "# extend: using vstack to stack 3 different arrays\n",
    "x_extend = np.hstack((x_-46*8,x_,x_+46*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the extended dataset\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(x_extend,y_extend,'b')\n",
    "plt.plot(x_,y_,'k+')\n",
    "plt.plot([0.,0.],[0.,2.5],'r')\n",
    "plt.plot([365.,365.],[0.,2.5],'r')\n",
    "plt.xlim(-356,2*365)\n",
    "plt.xlabel('day of year')\n",
    "plt.ylabel('LAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define xnew at 1 day interval\n",
    "xnew = np.arange(1.,366.)\n",
    "\n",
    "# linear interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "ynew = f(xnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cubic interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='cubic')\n",
    "ynew = f(xnew)\n",
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nearest neighbour interpolation\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='nearest')\n",
    "ynew = f(xnew)\n",
    "plt.plot(xnew,ynew)\n",
    "plt.plot(x_,y_,'r+')\n",
    "plt.xlim(1,366)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the problem you are trying to solve, different interpolation schemes will be appropriate. For categorical data (e.g. 'snow', coded as 1 and 'no snow' coded as 1), for instance, a nearest neighbour interpolation might be a good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with the schemes above is that they go exactly through the data points, but a more realistic description of the data might be one that incorporated the uncertainty information we have. Visually, this is quite easy to imagine, but how can we implement such ideas?\n",
    "\n",
    "One way of thinking about this is to think about other sources of information that we might bring to bear on the problem. One such would be that we expect the function to be 'quite smooth'. This allows us to consider applying smoothness as an additional constraint to the solution.\n",
    "\n",
    "Many such problems can be phrased as convolution operations.\n",
    "\n",
    "Convolution is a form of digital filtering that combines two sequences of numbers $y$ and $w$ to give a third, the result $z$ that is a filtered version of $y$, where for each element $j$ of $y$:\n",
    "\n",
    "$$\n",
    "  z_j = \\sum_{i=-n}^{i=n}{w_i y_{j+i}}\n",
    "$$\n",
    "\n",
    "where $n$ is the half width of the filter $w$. For a smoothing filter, the elements of this will sum to 1 (so that the magnitude of $y$ is not changed).\n",
    "\n",
    "To illustrate this in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple box smoothing filter\n",
    "# filter width 11\n",
    "w = np.ones(11)\n",
    "# normalise\n",
    "w = w/w.sum()\n",
    "# half width\n",
    "n = len(w)/2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "# where we will put the result\n",
    "z = np.zeros_like(y)\n",
    "\n",
    "# This is a straight implementation of the\n",
    "# equation above\n",
    "for j in xrange(n,len(y)-n):\n",
    "    for i in xrange(-n,n+1):\n",
    "        z[j] += w[n+i] * y[j+i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d'%len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suggested, the result of convolving $y$ with the filter $w$ (of width 31 here) is $z$, a smoothed version of $y$.\n",
    "\n",
    "You might notice that the filter is only applied once we are `n` samples into the signal, so we get 'edge effects'. There are various ways of dealing with edge effects, such as repeating the signal (as we did above, for much the same reason), reflecting the signal, or assuming the signal to be some constant value (e.g. 0) outside of its defined domain.\n",
    "\n",
    "If we make the filter wider (width 31 now):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a simple box smoothing filter\n",
    "# filter width 31\n",
    "w = np.ones(31)\n",
    "# normalise\n",
    "w = w/w.sum()\n",
    "# half width\n",
    "n = len(w)/2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "# where we will put the result\n",
    "z = np.zeros_like(y)\n",
    "\n",
    "# This is a straight implementation of the\n",
    "# equation above\n",
    "for j in xrange(n,len(y)-n):\n",
    "    for i in xrange(-n,n+1):\n",
    "        z[j] += w[n+i] * y[j+i]\n",
    "        \n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d'%len(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the signal is 'more' smoothed. \n",
    "\n",
    "There are *many* filters implemented in [`scipy.signal`](http://docs.scipy.org/doc/scipy/reference/signal.html) that you should look over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very commonly used smoothing filter is the [Savitsky-Golay](http://en.wikipedia.org/wiki/Savitzky–Golay_filter_for_smoothing_and_differentiation) filter for which you define the window size and filter order.\n",
    "\n",
    "As with most filters, the filter width controls the degree of smoothing (see examples above). The filter order (related to polynomial order) in essence controls the shape of the filter and defines the 'peakiness' of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "# see http://wiki.scipy.org/Cookbook/SavitzkyGolay\n",
    "from savitzky_golay import *\n",
    "\n",
    "window_size = 31\n",
    "order = 1\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "z = savitzky_golay(y,window_size,order)\n",
    "\n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d order %.2f'%(window_size,order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "# see http://wiki.scipy.org/Cookbook/SavitzkyGolay\n",
    "from savitzky_golay import *\n",
    "\n",
    "window_size = 61\n",
    "order = 2\n",
    "\n",
    "# Take the linear interpolation of the LAI above as the signal \n",
    "# linear interpolation\n",
    "x = xnew\n",
    "f = interpolate.interp1d(x_extend,y_extend,kind='linear')\n",
    "y = f(x)\n",
    "\n",
    "z = savitzky_golay(y,window_size,order)\n",
    "\n",
    "plt.plot(x,y,'k--',label='y')\n",
    "plt.plot(x,z,'r',label='z')\n",
    "plt.xlim(x[0],x[-1])\n",
    "plt.legend(loc='best')\n",
    "plt.title('smoothing with filter width %d order %.2f'%(window_size,order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the samples $y$ have uncertainty (standard deviation $\\sigma_j$ for sample $j$) associated with them, we can incorporate this into smoothing, although many of the methods in `scipy` and `numpy` do not directly allow for this.\n",
    "\n",
    "Instead, we call an optimal interpolation scheme (a regulariser) here that achieves this. This also has the advantage of giving an estimate of uncertainty for the smoothed samples.\n",
    "\n",
    "In this case, the parameters are: `order` (as above, but only integer in this implementation) and `wsd` which is an estimate of the variation (standard deviation) in the signal that control smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tile = 'h17v03'\n",
    "year = '2005'\n",
    "\n",
    "# specify the file with the urls in\n",
    "ifile= 'data/modis_lai_%s_%s.txt'%(tile,year)\n",
    "\n",
    "fp = open(ifile)\n",
    "filelist = [url.split('/')[-1].strip() for url in fp.readlines()]\n",
    "fp.close()\n",
    "import sys\n",
    "sys.path.insert(0,'files/python')\n",
    "\n",
    "from get_lai import *\n",
    "\n",
    "try:\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "except:\n",
    "    lai = read_lai(filelist,country='IRELAND')\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "    \n",
    "thresh = 0.25\n",
    "sd[sd<thresh] = thresh\n",
    "\n",
    "r = 472\n",
    "c = 84\n",
    "from smoothn import *\n",
    "\n",
    "# this is about the right amount of smoothing here\n",
    "gamma = 5.\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "z = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=2.0)[0]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "# lower and upper bounds of 95% CI\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.ylim(0.,2.5)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it on a new pixel\n",
    "\n",
    "r = 472\n",
    "c = 86\n",
    "\n",
    "gamma = 5\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "z = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=2.0)[0]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.legend(loc='best')\n",
    "z.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and test it on a new pixel\n",
    "\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "#r = 9\n",
    "#c = 277\n",
    "gamma = 5.\n",
    "\n",
    "pixel = data[:,r,c]\n",
    "pixel_sd =   sd[:,r,c]\n",
    "\n",
    "x = np.arange(46)*8+1\n",
    "\n",
    "order = 2\n",
    "# solve for gamma - degree of smoothness \n",
    "zz = smoothn(pixel,sd=pixel_sd,smoothOrder=2.0)\n",
    "z = zz[0]\n",
    "print zz[1],zz[2]\n",
    "\n",
    "gamma = zz[1]\n",
    "\n",
    "# plot\n",
    "plt.plot(x,pixel,'k*',label='y')\n",
    "plt.errorbar(x,pixel,pixel_sd*1.96)\n",
    "plt.plot(x,z,'r',label='z')\n",
    "\n",
    "plt.xlim(1,366)\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply this approach to our 3D dataset, we could simply loop over all pixels.\n",
    "\n",
    "Note that *any* per-pixel processing will be slow ... but this is quite a fast smoothing method, so is feasible here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have put in an axis control to smoothn\n",
    "# here so it will only smooth over doy\n",
    "# This will take a few minutes to process\n",
    "# we switch on verbose mode to get some feedback\n",
    "# on progress\n",
    "\n",
    "# make a mask of pixels where there is at least 1 sample\n",
    "# over the time period\n",
    "mask = (data.mask.sum(axis=0) == 0)\n",
    "mask = np.array([mask]*data.shape[0])\n",
    "\n",
    "z = smoothn(data,s=5.0,sd=sd,smoothOrder=2.0,axis=0,TolZ=0.05,verbose=True)[0]\n",
    "z = ma.array(z,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(z[20],interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarly, take frame 20\n",
    "# and smooth that\n",
    "\n",
    "ZZ = smoothn(z[20],smoothOrder=2.)\n",
    "# self-calibrated smoothness term\n",
    "s = ZZ[1]\n",
    "print 's =',s\n",
    "Z = ZZ[0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(Z,interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# similarly, take frame 20\n",
    "# and smooth that\n",
    "\n",
    "ZZ = smoothn(z,s=s,smoothOrder=2.,axis=(1,2),verbose=True)\n",
    "\n",
    "Z = ZZ[0]\n",
    "plt.figure(figsize=(9,9))\n",
    "plt.imshow(Z[30],interpolation='none',vmax=6)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(46)*8+1.\n",
    "try:\n",
    "    plt.plot(x,np.mean(Z,axis=(1,2)))\n",
    "    plt.plot(x,np.min(Z,axis=(1,2)),'r--')\n",
    "    plt.plot(x,np.max(Z,axis=(1,2)),'r--')\n",
    "except:\n",
    "    plt.plot(x,np.mean(Z,axis=2).mean(axis=1))\n",
    "    plt.plot(x,np.min(Z,axis=2).min(axis=1),'r--')\n",
    "    plt.plot(x,np.max(Z,axis=2).max(axis=1),'r--')\n",
    "    \n",
    "plt.title('LAI variation of Eire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or doing this pixel by pixel ...\n",
    "# which is slower than using axis\n",
    "\n",
    "order = 2\n",
    "\n",
    "# pixels that have some data\n",
    "mask = (~data.mask).sum(axis=0)\n",
    "\n",
    "odata = np.zeros((46,) + mask.shape)\n",
    "\n",
    "rows,cols = np.where(mask>0)\n",
    "\n",
    "len_x = len(rows)\n",
    "order = 2\n",
    "gamma = 5.\n",
    "\n",
    "for i in xrange(len_x):\n",
    "    r,c = rows[i],cols[i]\n",
    "    # progress bar\n",
    "    if i%(len_x/20) == 0:\n",
    "        print '... %4.2f percent'%(i*100./float(len_x))\n",
    "    pixel    = data[:,r,c]\n",
    "    pixel_sd = sd[:,r,c]\n",
    "\n",
    "    zz = smoothn(pixel,s=gamma,sd=pixel_sd,smoothOrder=order,TolZ=0.05)\n",
    "    odata[:,rows[i],cols[i]] = zz[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import os\n",
    "\n",
    "root = 'images/lai_eire_colourZ'\n",
    "\n",
    "for i,f in enumerate(lai['filenames']):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    # get some info from filename\n",
    "    file_id = f.split('/')[-1].split('.')[-5][1:]\n",
    "    print file_id\n",
    "    plt.imshow(Z[i],interpolation='none',vmax=6.,vmin=0.0)\n",
    "    # plot a jpg\n",
    "    plt.title(file_id)\n",
    "    plt.colorbar()\n",
    "    plt.savefig('%s_%s.jpg'%(root,file_id))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cmd = 'convert -delay 100 -loop 0 {0}_*.jpg {0}_movie2.gif'.format(root)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](files/images/lai_eire_colourZ_movie2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Function fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, instead of applying some arbitrary smoothing function to data, we want to extract particular infromation from the time series.\n",
    "\n",
    "One way to approach this is to fit some function to the time series at each location.\n",
    "\n",
    "Let us suppose that we wish to characterise the phenology of vegetation in Ireland.\n",
    "\n",
    "![](http://www2.geog.ucl.ac.uk/~plewis/geogg124/_images/zhang1.png)\n",
    "\n",
    "One way we could do this would be to look in the lai data for the most rapid changes.\n",
    "\n",
    "Another would be to explicitly fit some mathematical function to the LAI data that would would expect to descrive typical LAI trajectories.\n",
    "\n",
    "One example of such a function is the double logistic. A logistic function is:\n",
    "\n",
    "$$\n",
    " \\hat{y} = p_0 - p_1 \\left( \\frac{1}{1 + e^{p_2 (t - p_3)}} + \\frac{1}{1 + e^{p_4 (t - p_5)}} -1\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give a function for a double logistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dbl_logistic_model ( p, t ):\n",
    "        \"\"\"A double logistic model, as in Sobrino and Juliean, \n",
    "        or Zhang et al\"\"\"\n",
    "        return p[0] - p[1]* ( 1./(1+np.exp(p[2]*(t-p[3]))) + \\\n",
    "                              1./(1+np.exp(-p[4]*(t-p[5])))  - 1 )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tile = 'h17v03'\n",
    "year = '2005'\n",
    "\n",
    "# specify the file with the urls in\n",
    "ifile= 'data/modis_lai_%s_%s.txt'%(tile,year)\n",
    "\n",
    "fp = open(ifile)\n",
    "filelist = [url.split('/')[-1].strip() for url in fp.readlines()]\n",
    "fp.close()\n",
    "import sys\n",
    "sys.path.insert(0,'python')\n",
    "\n",
    "from get_lai import *\n",
    "\n",
    "try:\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "except:\n",
    "    lai = read_lai(filelist,country='IRELAND')\n",
    "    data = lai['Lai_1km']\n",
    "    sd = lai['LaiStdDev_1km']\n",
    "    \n",
    "thresh = 0.25\n",
    "sd[sd<thresh] = thresh\n",
    "\n",
    "# test pixel\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "\n",
    "y = data[:,r,c]\n",
    "mask = ~y.mask\n",
    "y = np.array(y[mask])\n",
    "x = (np.arange(46)*8+1.)[mask]\n",
    "unc = np.array(sd[:,r,c][mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define x (time)\n",
    "x_full = np.arange(1,366) \n",
    "\n",
    "# some default values for the parameters\n",
    "p = np.zeros(6)\n",
    "\n",
    "# some stats on y\n",
    "ysd = np.std(y)\n",
    "ymean = np.mean(y)\n",
    "\n",
    "# some rough guesses at the parameters\n",
    "\n",
    "p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "p[1] = 2*1.151*ysd          # range\n",
    "p[2] = 0.19                 # related to up slope\n",
    "p[3] = 120                  # midpoint of up slope\n",
    "p[4] = 0.13                 # related to down slope\n",
    "p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "y_hat = dbl_logistic_model(p,x_full)\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(x_full,y_hat)\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually 'tweak' the parameters until we got a better 'fit' to the observations.\n",
    "\n",
    "First though, let's define a measure of 'fit':\n",
    "\n",
    "$$\n",
    "Z_i = \\frac{\\hat{y}_i - y_i}{\\sigma_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^2 = \\sum_i{Z_i^2} =  \\sum_i{\\left( \\frac{\\hat{y}_i - y_i}{\\sigma_i} \\right)^2}\n",
    "$$\n",
    "\n",
    "and implement this as a mismatch function where we have data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mismatch_function(p, x, y, unc):\n",
    "    y_hat = dbl_logistic_model(p, x)\n",
    "    diff = (y_hat - y)/unc\n",
    "    return diff\n",
    "\n",
    "\n",
    "Z = mismatch_function(p,x,y,unc)\n",
    "\n",
    "plt.plot([1,365.],[0,0.],'k-')\n",
    "plt.xlim(0,365)\n",
    "plt.plot(x,Z,'*')\n",
    "\n",
    "\n",
    "print 'Z^2 =',(Z**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets change p a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "p[1] = 2*1.151*ysd          # range\n",
    "p[2] = 0.19                 # related to up slope\n",
    "p[3] = 140                  # midpoint of up slope\n",
    "p[4] = 0.13                 # related to down slope\n",
    "p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "Z = mismatch_function(p,x,y,unc)\n",
    "\n",
    "plt.plot([1,365.],[0,0.],'k-')\n",
    "plt.xlim(0,365)\n",
    "plt.plot(x,Z,'*')\n",
    "\n",
    "\n",
    "print 'Z^2 =',(Z**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have made the mismatch go down a little ...\n",
    "\n",
    "Clearly it would be tedious (and impractical) to do a lot of such tweaking, so we can use methods that seek the minimum of some function.\n",
    "\n",
    "One such method is implemented in `scipy.optimize.leastsq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# initial estimate is in p\n",
    "print 'initial parameters:',p[0],p[1],p[2],p[3],p[4],p[5]\n",
    "\n",
    "# set some bounds for the parameters\n",
    "bound = np.array([(0.,10.),(0.,10.),(0.01,1.),\\\n",
    "                  (50.,300.),(0.01,1.),(50.,300.)])\n",
    "\n",
    "\n",
    "# test pixel\n",
    "r = 472\n",
    "c = 84\n",
    "\n",
    "\n",
    "y = data[:,r,c]\n",
    "mask = ~y.mask\n",
    "y = np.array(y[mask])\n",
    "x = (np.arange(46)*8+1.)[mask]\n",
    "unc = np.array(sd[:,r,c][mask])\n",
    "\n",
    "# define function to give Z^2\n",
    "\n",
    "def sse(p,x,y,unc):\n",
    "    '''Sum of squared error'''\n",
    "    # penalise p[3] > p[5]\n",
    "    err = np.max([0.,(p[3] - p[5])])*1e4\n",
    "    return (mismatch_function(p,x,y,unc)**2).sum()+err\n",
    "\n",
    "# we pass the function:\n",
    "#\n",
    "# sse               : the name of the function we wrote to give \n",
    "#                     sum of squares of Z_i\n",
    "# p                 : an initial estimate of the parameters\n",
    "# args=(x,y,unc)    : the other information (other than p) that\n",
    "#                     mismatch_function needs\n",
    "# approx_grad       : if we dont have a function for the gradien\n",
    "#                     we have to get the solver to approximate it\n",
    "#                     which takes time ... see if you can work out\n",
    "#                     d_sse / dp and use that to speed this up!\n",
    "\n",
    "psolve = optimize.fmin_l_bfgs_b(sse,p,approx_grad=True,iprint=-1,\\\n",
    "                                args=(x,y,unc),bounds=bound)\n",
    "\n",
    "print psolve[1]\n",
    "pp = psolve[0]\n",
    "plt.plot(x,y,'*')\n",
    "plt.errorbar(x,y,unc*1.96)\n",
    "y_hat = dbl_logistic_model(pp,x_full)\n",
    "plt.plot(x_full,y_hat)\n",
    "\n",
    "print 'solved parameters: ',pp[0],pp[1],pp[2],pp[3],pp[4],pp[5]\n",
    "\n",
    "# if we define the phenology as the parameter p[3]\n",
    "# and the 'length' of the growing season:\n",
    "print 'phenology',pp[3],pp[5]-pp[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and run over each pixel ... this will take some time\n",
    "\n",
    "# pixels that have some data\n",
    "mask = (~data.mask).sum(axis=0)\n",
    "\n",
    "pdata = np.zeros((7,) + mask.shape)\n",
    "\n",
    "rows,cols = np.where(mask>0)\n",
    "len_x = len(rows)\n",
    "\n",
    "# lets just do some random ones to start with\n",
    "#rows = rows[::10]\n",
    "#cols = cols[::10]\n",
    "\n",
    "len_x = len(rows)\n",
    "\n",
    "\n",
    "for i in xrange(len_x):\n",
    "    r,c = rows[i],cols[i]\n",
    "    # progress bar\n",
    "    if i%(len_x/40) == 0:\n",
    "        print '... %4.2f percent'%(i*100./float(len_x))\n",
    "    \n",
    "    y = data[:,r,c]\n",
    "    mask = ~y.mask\n",
    "    y = np.array(y[mask])\n",
    "    x = (np.arange(46)*8+1.)[mask]\n",
    "    unc = np.array(sd[:,r,c][mask])\n",
    "    \n",
    "    # need to get an initial estimate of the parameters\n",
    "    \n",
    "    # some stats on y\n",
    "    ysd = np.std(y)\n",
    "    ymean = np.mean(y)\n",
    "\n",
    "    p[0] = ymean - 1.151*ysd;   # minimum  (1.151 is 75% CI)\n",
    "    p[1] = 2*1.151*ysd          # range\n",
    "    p[2] = 0.19                 # related to up slope\n",
    "    p[3] = 140                  # midpoint of up slope\n",
    "    p[4] = 0.13                 # related to down slope\n",
    "    p[5] = 220                  # midpoint of down slope\n",
    "\n",
    "    \n",
    "    # set factr to quite large number (relative error in solution)\n",
    "    # as it'll take too long otherwise\n",
    "    psolve = optimize.fmin_l_bfgs_b(sse,p,approx_grad=True,iprint=-1,\\\n",
    "                                args=(x,y,unc),bounds=bound,factr=1e12)\n",
    "\n",
    "    pdata[:-1,rows[i],cols[i]] = psolve[0]\n",
    "    pdata[-1,rows[i],cols[i]] = psolve[1] # sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[3],interpolation='none',vmin=137,vmax=141)\n",
    "plt.title('green up doy')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[5]-pdata[3],interpolation='none',vmin=74,vmax=84)\n",
    "plt.title('season length')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[0],interpolation='none',vmin=0.,vmax=6.)\n",
    "plt.title('min LAI')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(pdata[1]+pdata[0],interpolation='none',vmin=0.,vmax=6.)\n",
    "plt.title('max LAI')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.sqrt(pdata[-1]),interpolation='none',vmax=np.sqrt(500))\n",
    "plt.title('RSSE')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check a few pixels\n",
    "\n",
    "c = 200\n",
    "\n",
    "for r in xrange(200,400,25):\n",
    "    y = data[:,r,c]\n",
    "    mask = ~y.mask\n",
    "    y = np.array(y[mask])\n",
    "    x = (np.arange(46)*8+1.)[mask]\n",
    "    unc = np.array(sd[:,r,c][mask])\n",
    "    \n",
    "    x_full = np.arange(1,366) \n",
    "    \n",
    "    # some default values for the parameters\n",
    "    pp = pdata[:-1,r,c]\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.title('r %d c %d'%(r,c))\n",
    "    plt.plot(x,y,'*')\n",
    "    plt.errorbar(x,y,unc*1.96)\n",
    "    y_hat = dbl_logistic_model(pp,x_full)\n",
    "    plt.plot(x_full,y_hat)\n",
    "    \n",
    "    print 'solved parameters: ',pp[0],pp[1],pp[2],pp[3],pp[4],pp[5]\n",
    "    \n",
    "    # if we define the phenology as the parameter p[3]\n",
    "    # and the 'length' of the growing season:\n",
    "    print 'phenology',pp[3],pp[5]-pp[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
